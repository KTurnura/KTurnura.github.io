<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hudi-Hive-3.1.3</title>
    <url>/posts/c8e82d64/</url>
    <content><![CDATA[<p>关于Hudi 0.13.1 Streaming Query  报错</p>
<blockquote>
<p> org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat ClassNotFoundException</p>
</blockquote>
<p>Keywords: <strong>Hudi编译Hive</strong>，<strong>Flink对Hudi进行Streaming Query</strong>，<strong>Hudi包冲突</strong>，<strong>编译Hudi</strong></p>
<p>版本： </p>
<table>
<thead>
<tr>
<th>Hadoop</th>
<th>3.2.2</th>
</tr>
</thead>
<tbody><tr>
<td>Spark</td>
<td>3.2.4</td>
</tr>
<tr>
<td>Flink</td>
<td>1.16.1</td>
</tr>
<tr>
<td>Hive</td>
<td>3.1.3（后回退版本到3.1.2）</td>
</tr>
<tr>
<td>Hudi</td>
<td>0.13.1（后使用Hudi Github master分支-0.14.0）</td>
</tr>
</tbody></table>
<p>&#x3D;&#x3D;(此处有包冲突，即Hive用的guava.jar版本为guava-19.0.jar,而Hadoop 3.2.2 版本guava.jar版本为guava-27.0-jre.jar)&#x3D;&#x3D;</p>
<p>一般不建议runtime的Hadoop版本高于hive依赖的版本</p>
<h1 id="场景：-使用Flink-SQL-对Hudi表进行Streaming-Query"><a href="#场景：-使用Flink-SQL-对Hudi表进行Streaming-Query" class="headerlink" title="场景： 使用Flink SQL 对Hudi表进行Streaming Query"></a>场景： 使用Flink SQL 对Hudi表进行Streaming Query</h1><p>官网介绍：<a href="https://hudi.apache.org/docs/flink-quick-start-guide#streaming-query">使用streaming-query实时查询Hudi表流式数据</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE t2(</span><br><span class="line">  uuid VARCHAR(20) PRIMARY KEY NOT ENFORCED,</span><br><span class="line">  name VARCHAR(10),</span><br><span class="line">  age INT,</span><br><span class="line">  ts TIMESTAMP(3),</span><br><span class="line">  `partition` VARCHAR(20)</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (`partition`)</span><br><span class="line">WITH (</span><br><span class="line">  &#x27;connector&#x27; = &#x27;hudi&#x27;,</span><br><span class="line">  &#x27;path&#x27; = &#x27;hdfs://localhost:9000/hudi-warehouse/hudi-t1&#x27;,</span><br><span class="line">  &#x27;table.type&#x27; = &#x27;MERGE_ON_READ&#x27;,</span><br><span class="line">  &#x27;read.streaming.enabled&#x27; = &#x27;true&#x27;,  </span><br><span class="line">  &#x27;read.start-commit&#x27; = &#x27;20210316134557&#x27;, </span><br><span class="line">  &#x27;read.streaming.check-interval&#x27; = &#x27;4&#x27;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>当使用Flink Client SQL 对Hudi进行 Streaming Query 时，注意官网<code>Note</code></p>
<blockquote>
<p>The bundle jar with <strong>hive profile</strong> is needed for streaming query, by default the officially released flink bundle is built <strong>without</strong> <strong>hive profile</strong>, the jar needs to be built manually, see <a href="https://hudi.apache.org/docs/syncing_metastore#install">Build Flink Bundle Jar</a> for more details.</p>
</blockquote>
<p>然后我们此时就必须自己通过Hudi源码编译 <code>hudi-flink1.16-bundle-0.13.1.jar</code>包</p>
<p>下载Hudi-0.13.1 版本的源码，解压，修改参数编译</p>
<p>但当修改<code>~/hudi-0.13.1/pom.xml</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;hadoop.version&gt;3.1.2&lt;/hadoop.version&gt;</span><br><span class="line">&lt;hive.groupid&gt;org.apache.hive&lt;/hive.groupid&gt;</span><br><span class="line">&lt;hive.version&gt;3.1.3&lt;/hive.version&gt;</span><br></pre></td></tr></table></figure>

<p>以及	<code>~/hudi-0.13.1/packaging/hudi-flink-bundle/pom.xml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;hive.version&gt;3.1.3&lt;/hive.version&gt;</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">&lt;profile&gt;</span><br><span class="line">   &lt;id&gt;flink-bundle-shade-hive3&lt;/id&gt;</span><br><span class="line">   &lt;properties&gt;</span><br><span class="line">     &lt;hive.version&gt;3.1.3&lt;/hive.version&gt;</span><br><span class="line">     &lt;flink.bundle.hive.scope&gt;compile&lt;/flink.bundle.hive.scope&gt;</span><br><span class="line">   &lt;/properties&gt;</span><br><span class="line">   &lt;dependencies&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">       &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt;</span><br><span class="line">       &lt;artifactId&gt;hive-service-rpc&lt;/artifactId&gt;</span><br><span class="line">       &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">       &lt;scope&gt;$&#123;flink.bundle.hive.scope&#125;&lt;/scope&gt;</span><br><span class="line">     &lt;/dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">       &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt;</span><br><span class="line">       &lt;artifactId&gt;hive-standalone-metastore&lt;/artifactId&gt;</span><br><span class="line">       &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">       &lt;scope&gt;$&#123;flink.bundle.hive.scope&#125;&lt;/scope&gt;</span><br><span class="line">     &lt;/dependency&gt;</span><br><span class="line">   &lt;/dependencies&gt;</span><br><span class="line"> &lt;/profile&gt;</span><br></pre></td></tr></table></figure>

<p>然后</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mvn clean install -DskipTests -Pflink-bundle-shade-hive3</span><br></pre></td></tr></table></figure>

<p>时无法构建，报错</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[INFO] hudi-hadoop-mr ..................................... FAILURE [  2.121 s]</span><br><span class="line">.....</span><br><span class="line">[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project hudi-hadoop-mr: Compilation failure: Compilation failure: </span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java:[92,50] 无法将类 org.apache.hudi.hadHoodieParquetInputFormat中的构造器 HoodieParquetInputFormat应用到给定类型;</span><br><span class="line">[ERROR]   需要: org.apache.hudi.hadoop.HoodieCopyOnWriteTableInputFormat</span><br><span class="line">[ERROR]   找到: 没有参数</span><br><span class="line">[ERROR]   原因: 实际参数列表和形式参数列表长度不同</span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java:[846,14] 无法将类 org.apache.huadoop.HoodieParquetInputFormat中的构造器 HoodieParquetInputFormat应用到给定类型;</span><br><span class="line">[ERROR]   需要: org.apache.hudi.hadoop.HoodieCopyOnWriteTableInputFormat</span><br><span class="line">[ERROR]   找到: 没有参数</span><br><span class="line">[ERROR]   原因: 实际参数列表和形式参数列表长度不同</span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java:[302,93] 不兼容的类型: org.apache.hadoopcommon.type.Date无法转换为java.sql.Date</span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java:[305,72] 不兼容的类型: org.apache.hadoopcommon.type.Timestamp无法转换为java.sql.Timestamp</span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HiveAvroSerializer.java:[309,98] 不兼容的类型: org.apache.hadoopcommon.type.Date无法转换为java.sql.Date</span><br><span class="line">[ERROR] /Users/kturnura/Packages/hudi-0.13.1/hudi-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java:[26,8] 无法将类 org.apache.hudi.hadoop.HoodiePatInputFormat中的构造器 HoodieParquetInputFormat应用到给定类型;</span><br></pre></td></tr></table></figure>

<h1 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h1><p>我们在Github找到了本问题的原因 <a href="https://github.com/apache/hudi/pull/7173">https://github.com/apache/hudi/pull/7173</a></p>
<p>时间戳无法被Hive3 获取</p>
<h2 id="解决方法一（提供思路，未能尝试成功）"><a href="#解决方法一（提供思路，未能尝试成功）" class="headerlink" title="解决方法一（提供思路，未能尝试成功）"></a>解决方法一（提供思路，未能尝试成功）</h2><p>使用patch 合并Contributor 的代码 <a href="https://github.com/apache/hudi/files/11574950/5189.patch.zip%E5%88%B0%E6%9C%AC%E5%9C%B0%E6%BA%90%E7%A0%81%E4%B8%AD">https://github.com/apache/hudi/files/11574950/5189.patch.zip到本地源码中</a></p>
<p>测试使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git apply --check patch/5189.patch</span><br></pre></td></tr></table></figure>

<p>合并到hudi-0.13.1 源码中打补丁失败</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git apply --reject file.patch</span><br></pre></td></tr></table></figure>

<p>手动合并，使用这个命令产生冲突无法合并的文件会在<code>~/hudi-0.13.1/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop</code>生成 .rej文件，然后手动合并</p>
<p>可以以下链接参照来逐一修改java文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/apache/hudi/pull/7173/files#diff-cd42bec5495e839654687e99670f40d27defe7178bdd347f23db651fc3262bb7 </span><br></pre></td></tr></table></figure>

<blockquote>
<p> 此处比较迷惑的点是由于Git，使用Hudi-0.13.1源码合并Hudi master分支下的patch包。</p>
<p> 还有一个就是尝试过直接copy master分支代码到Hudi 0.13版本在hudi-hadoop-mr&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;hudi&#x2F;hadoop的代码，但是有新代码添加，无法成功执行</p>
</blockquote>
<h2 id="解决办法-二"><a href="#解决办法-二" class="headerlink" title="解决办法 二"></a>解决办法 二</h2><p>使用最新源码，同时回退Hive-3.1.2（这是出现另一个问题时Pflink-bundle-shade-hive3 默认指定hive版本为3.1.2，本人尝试多种方式解决减少冲突时回退了版本，&#x3D;&#x3D;但使用该方法使用 hive 3.1.3 应该也可以解决（因为源码中关于Hive 3.1.3 的时间戳）&#x3D;&#x3D;）</p>
<p>出现问题：</p>
<ol>
<li><p>查看许多博客的方法：修改Hudi源码 <code>pom.xml</code>以及<code>~/hudi/packaging/hudi-flink-bundle/pom.xml</code> 中<code>hadoop.version</code>和<code>hive.version</code>版本。但仍然会出现guava.jar 冲突。报错：</p>
<blockquote>
<p>Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument (ZLjava&#x2F;lang&#x2F;String;Ljava&#x2F;lang&#x2F;Object;)V.</p>
</blockquote>
<p>原因：经过检查发现Hadoop3.1.3 和 hive-3.1.2 的lib下面的guava都是guava-27.0-jre.jar，没有冲突问题。<br>那就说明只可能是flink&#x2F;lib下面添加的上面4个jar包中有jar包编译时，引用的guava版本过低导致的。<br>通过源码锁定了hive-exec-3.1.2.jar 引用了guava-19.0.jar 包,并且 flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar 引用了 hive-exec-3.1.2.jar &#x3D;&#x3D;参考文献&#x3D;&#x3D;：<a href="https://blog.csdn.net/chenshijie2011/article/details/114373468">https://blog.csdn.net/chenshijie2011/article/details/114373468</a></p>
</li>
<li><p>通过上述参考文献编译<code>Hive-exec-3.1.2.jar</code> 源码并没有解决问题，依然存在冲突</p>
</li>
</ol>
<p>解决方式&#x3D;&#x3D;参考文献&#x3D;&#x3D;： <a href="https://www.cnblogs.com/itxiaoshen/p/16934160.html">https://www.cnblogs.com/itxiaoshen/p/16934160.html</a></p>
<p>直接编译文件：具体版本根据个人环境调参</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests -Dcheckstyle.skip -Dspark3.2 -Dflink1.16 -Dscala-2.12 -Dhadoop.version=3.2.2 -Pflink-bundle-shade-hive3</span><br></pre></td></tr></table></figure>

<p>此时打出的jar包中guava.jar 版本依然为19.0 ,而此时hadoop 的guava.jar 包中的 版本为 <code>guava-27.0-jre.jar</code></p>
<p>需要拷贝hadoop下的该jar包到flink&#x2F;lib目录下！！</p>
<p>这是最简单的方式！！！！</p>
<h1 id="有感"><a href="#有感" class="headerlink" title="有感"></a>有感</h1><p>该文后续语言比较混乱，有大量试错的过程，原因是在解决该问题的同时用本文记录解决过程，后经过一次简单修改发布了本博客。该问题其实解决方式比较简单，只需要下载Hudi最新源码，编译Hudi，然后复制 guava.jar 到flink&#x2F;lib目录下即可解决问题。但这个解决问题的过程收获颇多。在编译Hive 和Hudi 诸多版本的源码时也遇到了一些其他的问题，解决一个问题是一个复杂问题的集合。</p>
<p>希望该文能帮到各位</p>
]]></content>
      <categories>
        <category>Data Lake</category>
      </categories>
      <tags>
        <tag>Hudi</tag>
      </tags>
  </entry>
  <entry>
    <title>text.pdf</title>
    <url>/posts/cec93071/</url>
    <content><![CDATA[<center> **测试**</center>

<br>

<div class="pdf-container" data-target="/pdf/CAP_theorem.pdf" data-height="500px"></div>

</br>
]]></content>
  </entry>
  <entry>
    <title>分布式事务</title>
    <url>/posts/cdcc4eae/</url>
    <content><![CDATA[<ol>
<li><p>在遇上并发的时候，我们需要一个评价并发结果是否正确的标准</p>
<p>数据库通常称这种标准为<strong>ACID</strong></p>
<ol>
<li>Atomic</li>
<li>Consistent</li>
<li>Isolated ： 两个事务同时运行，彼此看不到对方的中间更新操作的值<ol>
<li>引出了一个性质：Serializable</li>
</ol>
</li>
<li>Durable： 事务提交之后，事务所做的修改是持久的，不会因为某些故障而丢失。</li>
</ol>
</li>
<li><p>事务可能会遇到事务中止的情况</p>
<ol>
<li>事务没有找到某个记录</li>
<li>除零异常</li>
<li>出现死锁，需要干掉引起死锁的事务</li>
</ol>
</li>
<li><p>提供有序性的重要工具</p>
<ol>
<li>Concurrency Control</li>
<li>原子提交</li>
</ol>
</li>
<li><p>并发控制的两种方案</p>
<ol>
<li><p>悲观锁并发控制</p>
<p>在事务使用任何数据之前，他需要先去获取该数据所对应的锁（<strong>不是一次获取该事务所执行的所有锁！！！</strong>）</p>
<p>但会导致延迟，可能会存在FIFO类事务处理顺序，无法导致并发</p>
<p><strong>如果经常发生冲突，推荐使用悲观锁</strong></p>
</li>
<li><p>乐观锁并发控制（OOC） </p>
<p>不需要担心是否有其他事务和我们同时对数据进行读取，如果遇到并发冲突，那么你就会遇上一大堆事务中止的情况</p>
</li>
</ol>
<p>	 </p>
</li>
<li><p>本文中主要讲的悲观锁，使用两阶段锁(2PL)的方式</p>
<ol>
<li>在对任何数据进行读取和写入操作之前，我们先去获取该数据对应的锁，直到事务被提交或者终止后，事务才能释放掉它所获取的锁</li>
<li>悲观锁可能会导致死锁的情况 <ol>
<li>数据库可以通过超时机制、跟踪周期等方案检测出死锁问题</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>事务不能提前释放某个之前使用的锁</strong></p>
<ol>
<li><p>提前释放锁可能会导致某个事务结果错误</p>
<img src="https://raw.githubusercontent.com/KTurnura/imgs/main/202311/image-20231121104340443.png" style="zoom: 33%;" />


</li>
<li><p>同时，提前释放锁，可能该事务中止后，另一个获取该锁的事务会出现幻读的情况</p>
</li>
</ol>
</li>
</ol>
<h1 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h1><ol>
<li><p>原子性，要么执行所有操作，要么就都不执行</p>
</li>
<li><p>原子提交协议Atomic commit protocol</p>
<ol>
<li>用来帮助这些电脑判断他们是否能够完成他们自己那部分的任务并真正执行该部分任务。</li>
</ol>
</li>
<li><p>存在一个Transaction Coordinate 的角色，向参与者发送执行信息。负责执行事务中部分任务的服务器叫做参与者，参与者之间可能持有不同的数据（<strong>有别于分布式系统为了保证容错而复制完整数据的行为</strong>）</p>
</li>
<li><p>每个服务器都保存不同事务各自持有的锁，<strong>不同事务通过Transaction ID来区分</strong>， </p>
</li>
<li><p>两阶段提交的流程 </p>
<img src="https://raw.githubusercontent.com/KTurnura/imgs/main/202311/image-20231120204516881.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/KTurnura/imgs/main/202311/image-20231121111352956.png" style="zoom:50%;" />

<p> 在worker执行完commit，向coordinate恢复ack信息的时候，会释放锁</p>
</li>
</ol>
<h1 id="事务执行中断后，分布式事务如何进行恢复"><a href="#事务执行中断后，分布式事务如何进行恢复" class="headerlink" title="事务执行中断后，分布式事务如何进行恢复"></a>事务执行中断后，分布式事务如何进行恢复</h1><ol>
<li>如果B在给TC发送yes信息后崩溃， 因为TC收到了A、B的yes信息，则TC选择提交分布式事务，  A就会执行部分分布式任务，将部分结果持久化到磁盘，并最终释放锁</li>
</ol>
<p>​	当B崩溃恢复时，他依然准备去完成他之前准备负责的那部分事务 </p>
<p>​	他并不知道任何事情，因为他还没有收到commit消息</p>
<p>​	服务器不能因为崩溃和重启而丢失一个事务的状态</p>
<p>​	<strong>在B回复TC的prepare消息之前</strong>，她首先得将该事务在内存中事务锁管理数	据的中间状态（即它所做的这些修改）和持有的lock列表持久化到磁盘上，	以 防止可能发生的崩溃中止，以及该事务持有的所有锁的记录持久化到磁盘	上，如果发生中止 ，那就只讲所相关的记录持久化到磁盘上（<strong>这也是造	成二阶段提交速度有点儿慢的原因所在</strong>）	</p>
<p>​	 当他重启后，他就会进行恢复，然后重新发送prepared消息</p>
<ol start="2">
<li>同理，如果在接受到commit信息之后崩溃，可能B首先先将所有事务所需要的中间状态和锁持久化后，才会真的开始执行该事务，如果收到一个内存中没有的事务COMMIT消息，则该worker会对TC进行反馈</li>
</ol>
<h1 id="TC-错误"><a href="#TC-错误" class="headerlink" title="TC 错误"></a>TC 错误</h1><ol>
<li><p>如果TC给A发送COMMIT消息之后崩溃了，那么TC必须进行重启并重新发送该commit消息，以确保两方都知道该事务可以被提交</p>
<p>TC在发送任何commit消息之前，TC首先将关于该事务的信息（结果和事务id）写入到他的日志中，并持久化到存储设备中，比如磁盘，之后才发送commit信息</p>
<p><strong>所以这也是Worker准备好接受重复commit消息的原因</strong></p>
</li>
<li><p>如果在发送commit消息之前崩溃，则没有关系，因为TC可以直接中止该事务，如果有某个参与者询问该事务的相关消息，参与者检查自己的日志，发现并没有commit信息，TC就会告诉他，我并不知道该事务的相关信息，那么该事务必然被终止了</p>
</li>
</ol>
<h1 id="网络传播信息丢失"><a href="#网络传播信息丢失" class="headerlink" title="网络传播信息丢失"></a>网络传播信息丢失</h1><ol>
<li><p>重发信息，如果有个服务器完全没有回复信息</p>
<p>那么TC会选择不在该组服务器上运行该事务</p>
<p>因为此时，回复yes&#x2F;no的服务器持有该事务所需要的锁，会对其他事务造成影响</p>
<p>将abort消息转发给Worker</p>
<p>如果后续未回复的服务器恢复，他会向TC反馈并没有接受到有关某个事务的信息，事务协调器此时内存中已经没有了关于该事务的任何信息，因为事务已经被TC中止，并将该事务的状态从它所保存的表中移除了，此时TC也会告诉worker，你也应该讲该事务中止</p>
</li>
<li><p>如果Worker在等待PREPARE的消息的过程中超时，则我们始终会允许该worker去中止该事务</p>
</li>
<li><p>但是如果在接受PREPARE消息，并恢复YES&#x2F;NO之后，此时TC因为网络延迟等原因掉线，Worker一直等待COMMIT消息，<strong>则此时WORKER不能自己选择中止该进程</strong>，即使超时了，阻塞所有其他事务，也必须等待</p>
<p>因为此时TC可能已经提交了本次事务，其他服务器已经提交了，将本次事务所做的修改落地，并释放了锁，将这些修改展示给其他事务</p>
<p>这种情况，就得让人对TC进行修复，并让他重启，读取上面它所保存的日志，最后发送了延迟很久都没发送的commit消息</p>
</li>
</ol>
<h1 id="阻塞特性"><a href="#阻塞特性" class="headerlink" title="阻塞特性"></a>阻塞特性</h1><p>阻塞特性是2PC协议带来的属性，很难解决</p>
<p>人们努力的将Worker接受到PREPARE到接收COMMIT消息的这段处理速度尽可能快，</p>
<img src="https://raw.githubusercontent.com/KTurnura/imgs/main/202311/image-20231121111352956.png" style="zoom:50%;" />

<p>这样，故障导致锁被参与者长时间持有而阻塞其他事务的时间就会尽可能的短</p>
<p>甚至在该协议的某些变种，可能不需要去等待</p>
<h1 id="事务协调器的日志"><a href="#事务协调器的日志" class="headerlink" title="事务协调器的日志"></a>事务协调器的日志</h1><ol>
<li><p>TC必须将事务的有关信息保存在他的日志中，万一他崩溃了，这样就可以避免事务丢失，</p>
<p>当事务协调器忘记它日志中有关事务的信息时，会发生神么？</p>
<p>他会试着获取参与者所发送的所有确认信息，那么他就会知道所有的参与者是都提交了这个事务，还是中止了这个事务，所有的参与者都知道这个事务的命运</p>
<p>一旦接受到所有参与者的确认信息，那么事务协调器会擦除该事务的所有信息</p>
</li>
<li><p>一旦当参与者收到了commit或abort信息，并且他们已经执行完它们所负责的那部分事务，并将他们的更新落地，释放他们所持有的锁，当参与者发送确认消息给事务协调器后，他们也可以完全忘记该事务的有关信息</p>
</li>
</ol>
<h1 id="原子性的2PC协议能够保证ACID特性的事务"><a href="#原子性的2PC协议能够保证ACID特性的事务" class="headerlink" title="原子性的2PC协议能够保证ACID特性的事务"></a>原子性的2PC协议能够保证ACID特性的事务</h1><p>但由于太多轮网络传播和持久化操作</p>
<p>只会在很小的范围内看到有人使用二阶段提交，比如一个机房，或者一个小组织（银行内部）</p>
<p>类似于跨行转账，则不允许使用2PC，因为你不会想将你的数据库命运放在其他组织的手上</p>
<p>如果面向的事务特性不太需要某阶段的持久化， 那么可以缩减2PC的持久化过程</p>
<h1 id="区分Raft和2PC"><a href="#区分Raft和2PC" class="headerlink" title="区分Raft和2PC"></a>区分Raft和2PC</h1><p>Raft 不同服务器做的是相同的事情</p>
<p>2PC不同服务器做的是不同的事情</p>
<h1 id="是否可以将Raft和2PC结合构建一个系统-Lab4"><a href="#是否可以将Raft和2PC结合构建一个系统-Lab4" class="headerlink" title="是否可以将Raft和2PC结合构建一个系统 Lab4"></a>是否可以将Raft和2PC结合构建一个系统 Lab4</h1><p>该系统既具备raft进行复制所带来的高可用性， 也拥有在使用2PC是，让参与者去执行他们所负责的事务的这种能力</p>
<img src="https://raw.githubusercontent.com/KTurnura/imgs/main/202311/image-202311211619258592.png" style="zoom: 33%;" />



































]]></content>
      <categories>
        <category>Distributed Transaction</category>
      </categories>
      <tags>
        <tag>Distributed System</tag>
        <tag>分布式理论</tag>
      </tags>
  </entry>
  <entry>
    <title>编译Hive报错</title>
    <url>/posts/fd662788/</url>
    <content><![CDATA[<h1 id="编译Hive-报错"><a href="#编译Hive-报错" class="headerlink" title="编译Hive 报错"></a>编译Hive 报错</h1><blockquote>
<p>Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from&#x2F;to conjars (<a href="https://conjars.org/repo">https://conjars.org/repo</a>)</p>
</blockquote>
<p>找不到<code>pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar</code>包</p>
<p>原因：官网地址变动<a href="https://conjars.wensel.net/">conjars</a></p>
<p>解决办法：</p>
<p>修改Maven setting.xml文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;mirrors&gt;</span><br><span class="line">  &lt;mirror&gt;</span><br><span class="line">    &lt;id&gt;conjars&lt;/id&gt;</span><br><span class="line">    &lt;name&gt;conjars&lt;/name&gt;</span><br><span class="line">    &lt;url&gt;https://conjars.wensel.net/repo/&lt;/url&gt;</span><br><span class="line">    &lt;mirrorOf&gt;conjars&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;/mirror&gt;</span><br><span class="line">&lt;/mirrors&gt;</span><br></pre></td></tr></table></figure>

<p>此时构建就可成功</p>
<p>若没有访问外网的方式</p>
<p>可以参考以下博客</p>
<p><a href="https://blog.csdn.net/zxctime/article/details/106007508">https://blog.csdn.net/zxctime/article/details/106007508</a></p>
]]></content>
  </entry>
</search>
